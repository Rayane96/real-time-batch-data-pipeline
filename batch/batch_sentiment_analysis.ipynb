

!pip install streamlit
!pip install transformers accelerate sentencepiece


!pip install matplotlib
#===============================================================
# 1. LOAD CONFIGURATION AND INITIALISE REDDIT API
#=================================================================
import json
import praw

# Load credentials from config.json (safer than hardcoding)
with open("config.json") as f:
    cfg = json.load(f)

# Create Reddit API object
reddit = praw.Reddit(
    client_id=cfg["client_id"],
    client_secret=cfg["client_secret"],
    username=cfg["username"],
    password=cfg["password"],
    user_agent=cfg["user_agent"]
)

print("Reddit API authenticated.")

# =========================================================
# 2.Data Collection from Reddit > JSONL
# =========================================================

# a. SCRAPE REDDIT POSTS (BATCH MODE)

import json
import time
import os

# SELECTED SUBREDDITS FOR DATA COLLECTION
subreddits = [
    "climate",
    "climatechange",
    "sustainability",
    "GlobalClimateChange",
    "environment",
    "environmental_science",
    "climate_science",
    "renewableenergy",
    "climateaction",
    "climatepolicy",
    "climatecrisis",
    "futurology",
    "climatecrisis",
    "climateskeptics",
    "climatesolutions"
]

# We scrape 4 categories per subreddit to collect more posts
categories = ["hot", "new", "top", "rising"]

#  CREATE DIRECTORY FOR RAW DATA

# step 1: chosen directory (folder name and file)
output_file = "data/raw/climate_raw.jsonl"
#step 2: directory creation 
os.makedirs("data/raw", exist_ok=True)

# SCRAPING LOOP:
# # We use "a" (append) to keep adding new posts without overwriting old ones.
with open(output_file, "a", encoding="utf-8") as f:     # (UTF-8 makes sure all characters in Reddit posts are saved correctly)  
    for sub in subreddits:
        print(f"ðŸ“¡ Collecting from r/{sub} ...")        # which subreddit the scraper is currently collecting from
        for cat in categories:
            print(f"  â†’ {cat}")                          # Shows which category (hot/new/top/rising) is being collected
            try:
                for post in getattr(reddit.subreddit(sub), cat)(limit=800): # getattr dynamically calls: subreddit.hot(), subreddit.new(), etc.
                    rec = {
                        "subreddit": sub,
                        "title": post.title,
                        "text": post.selftext,
                        "score": int(post.score),         # From each posts: sub name, title, text, score, time created as python dictionary 
                        "created_utc": int(post.created_utc)
                            }
                    # Save each post as one JSON line (JSONL format)
                    f.write(json.dumps(rec, ensure_ascii=False) + "\n") # Convert python dictionary post to json line 
                    time.sleep(0.05)                                    # Small pause to avoid API rate limits
            except Exception as e:
                print(f"     Error in {sub}/{cat}: {e}")
                continue
print("âœ… Finished collecting raw data.") 

# =========================================================
# 3. LOAD RAW DATA INTO SPARK
# =========================================================



# Loading data to spark DataFrame : 
from pyspark.sql import SparkSession

# Start Spark session
spark = SparkSession.builder.appName("ClimateBatch").getOrCreate()

# Load raw data
df_raw = spark.read.json("data/raw/climate_raw.jsonl")

# Print schema
df_raw.printSchema()

# Data Inspection : Showing the count and sample of raw data BEFORE preprocessing satge 

# Number of rows before data preprocessing  
print("Number of rows (before cleaning):", df_raw.count())
# Sample of raw data before data preprocessing 
df_raw.show(20, truncate=False)


#----------------------------------- INSPECT------------------------------------------------------------------

from pyspark.sql import functions as F
#Data Inspection : Checking title and text content 
df_raw.filter(F.length(F.trim("text")) == 0).count() # # 23081

# 23081 posts have empty text

# =========================================================
# 4. BASIC CLEANING: trim, dedupe, remove empty
# =========================================================



from pyspark.sql import functions as F

# -------------------- 1. Normalize text --------------------
df_clean = (
    df_raw
    .withColumn("title_norm", F.trim(F.regexp_replace("title", r"\s+", " ")))
    .withColumn("text_norm",  F.trim(F.regexp_replace("text",  r"\s+", " ")))
)

# -------------------- 2. Drop duplicates --------------------
df_step1 = df_clean.dropDuplicates(["title_norm", "text_norm"])
print("After Drop duplicates:", df_step1.count())

# -------------------- 3. Remove empty rows --------------------
df_step1 = df_step1.filter(
    (F.length("title_norm") > 0) |
    (F.length("text_norm") > 0)
)
print("After Remove empty rows:", df_step1.count())

# -------------------- 4. DROP helper columns (keep only real fields) --------------------
df_step1 = df_step1.drop("title_norm", "text_norm")

# -------------------- 5. Show cleaned data  --------------------
df_step1.show(60, truncate=False)



# ------------------------------ INSPECT DATAFRAME with new column body_raw-------------------------------------------

# 4. creating helper column: body_raw (text+title) for performing sentimnetnat later one - NLP 
from pyspark.sql import functions as F

df1 = df_step1.withColumn(
    "body_raw",
    F.concat_ws(" ", "title", "text")
)

#df1.select("body_raw").show(60, truncate=False)
df1.show()
# =========================================================
# 5. DEEP CLEANING (remove URLs, emojis, markdown, etc)
# =========================================================


from pyspark.sql import functions as F

# -------------------- EMOJI RANGES --------------------
emoji_ranges = (
    r"\u2600-\u27BF"
    r"\u1F300-\u1F6FF"
    r"\u1F900-\u1F9FF"
    r"\u1FA70-\u1FAFF"
    r"\u1F1E6-\u1F1FF"
    r"\uFE0F"
)

# ASCII emoticons to keep
ascii_emoji_regex = r"(:\)|:\(|:-\)|:-\(|;\)|;-\)|:D|:-D)"

# Kaomoji to remove
kaomoji_regex = r"[\(\[][^A-Za-z0-9]{2,20}[\)\]]"


# ------------------------ CLEANING PIPELINE --------------------------

df_sent = (
    df1
    # Remove URLs
    .withColumn("body_sent", F.regexp_replace("body_raw", r"http\S+|www\.\S+", ""))

    # Remove markdown links [text]
    .withColumn("body_sent", F.regexp_replace("body_sent", r"\[.*?\]", ""))

    # Remove ">" quote markers
    .withColumn("body_sent", F.regexp_replace("body_sent", r">\s*", " "))

    # Remove escaped \n \t \r
    .withColumn("body_sent", F.regexp_replace("body_sent", r"\\n|\\t|\\r", " "))

    # Remove actual newlines
    .withColumn("body_sent", F.regexp_replace("body_sent", r"[\r\n]+", " "))

    # Remove long separators ___ or --- 
    .withColumn("body_sent", F.regexp_replace("body_sent", r"(([_\-]\s*){3,})", " "))
    .withColumn("body_sent", F.regexp_replace("body_sent", r"((\u2013|\u2014|\u2015)\s*){3,}", " "))

    # Remove @handles
    .withColumn("body_sent", F.regexp_replace("body_sent", r"@\s*[A-Za-z0-9_.-]+", " "))

    # Remove Kaomoji
    .withColumn("body_sent", F.regexp_replace("body_sent", kaomoji_regex, " "))

    # Remove unwanted characters EXCEPT: letters, digits, whitespace, ! ? . $ %
    .withColumn(
        "body_sent",
        F.regexp_replace(
            "body_sent",
            rf"[^\w\s!\?\.%\${emoji_ranges}]",
            " "
        )
    )

    # Keep ASCII emoticons
    .withColumn("body_sent",
        F.regexp_replace("body_sent", ascii_emoji_regex, r"\1")
    )

    # Normalize spaces
    .withColumn("body_sent", F.regexp_replace("body_sent", r"\s+", " "))
    .withColumn("body_sent", F.trim("body_sent"))
)

# Drop empty cleaned posts
df_sent = df_sent.filter(F.length("body_sent") > 1)

df_sent.select("body_raw", "body_sent").show(60, truncate=False)

#-------------------------INSPECT DATAFRAME with new identifiable column 


#Add post_id for chunk tracking
df_ready = df_sent.withColumn("post_id", F.col("created_utc"))
df_ready.show()

# =========================================================
# 6. CHUNKING FOR TRANSFORMERS
# =========================================================


# Sentence-based chunking for Model 
import re
from pyspark.sql.types import ArrayType, StringType

CHUNK_WORDS = 450

def chunk_text_sentences(text, size=CHUNK_WORDS):
    if text is None:
        return []

    # Split into sentences (simple but effective)
    sentences = re.split(r'(?<=[.!?])\s+', text)

    chunks = []
    current_chunk = []
    current_length = 0

    for s in sentences:
        word_count = len(s.split())

        # if adding this sentence exceeds limit â†’ start new chunk
        if current_length + word_count > size:
            if current_chunk:
                chunks.append(" ".join(current_chunk))
            current_chunk = [s]
            current_length = word_count
        else:
            current_chunk.append(s)
            current_length += word_count

    # append last chunk
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# Register UDF
chunk_udf = F.udf(chunk_text_sentences, ArrayType(StringType()))

# Apply UDF
df_chunked = df_ready.withColumn("chunks", chunk_udf(F.col("body_sent")))

# Explode to rows
df_chunks = (
    df_chunked
    .withColumn("chunk_text", F.explode("chunks"))
    .withColumn("chunk_id", F.monotonically_increasing_id())
    .select("post_id", "chunk_id", "chunk_text")
)

df_chunks.withColumn("word_count", F.size(F.split("chunk_text", " ")))
#.show(50, truncate=False)

df_chunks.show()
#-------------------------INSPECT ---------------------------------------------------
df_chunks.count()


# =========================================================
# 7. Transformer Inference (GPU) - check
# =========================================================
import torch
torch.cuda.is_available(), torch.cuda.get_device_name(0)

# =================================================================
# 8. GPU-Accelerated Transformer Inference for Sentiment Scoring
# =================================================================

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import time   

# ------------------------------------------------------------
# Load RoBERTa Model
# ------------------------------------------------------------
model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name).to("cuda")

# ------------------------------------------------------------
# Classification Function (GPU)
# ------------------------------------------------------------
def classify(text):
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length"
    ).to("cuda")

    with torch.no_grad():
        logits = model(**inputs).logits

    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()
    labels = ["negative", "neutral", "positive"]
    return labels[probs.argmax()]

# ------------------------------------------------------------
# 4) Convert Spark chunks â†’ Pandas for GPU processing
# ------------------------------------------------------------
pdf_chunks = df_chunks.toPandas()

# ------------------------------------------------------------
# 5) RUN GPU SENTIMENT WITH TIMER
# ------------------------------------------------------------

sentiments = []

print("Starting GPU inference...")

# START TIMER HERE â¬‡â¬‡â¬‡
start_time = time.time()

for text in pdf_chunks["chunk_text"]:
    sentiments.append(classify(text))

# END TIMER HERE â¬†â¬†â¬†
end_time = time.time()

# Print performance results
total_time = end_time - start_time
avg_time = total_time / len(pdf_chunks)

print("=================================================")
print(f"Total GPU inference time: {total_time:.2f} seconds")
print(f"Chunks processed: {len(pdf_chunks)}")
print(f"Average time per chunk: {avg_time:.4f} seconds")
print("=================================================")

# ------------------------------------------------------------
# 6) Add Sentiment + Convert back to Spark
# ------------------------------------------------------------
pdf_chunks["sentiment"] = sentiments
df_scored = spark.createDataFrame(pdf_chunks)

# ------------------------------------------------------------
# 7) Show final GPU-scored results
# ------------------------------------------------------------
df_scored.show(20, truncate=False)

# =================================================================
# 8.  Roberta Model : GPU-Accelerated Transformer Inference for Sentiment Scoring
# =================================================================

import pandas as pd
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

#-------------------------------------------
#  FIXED: use unique variable names for RoBERTa
#  (Your issue before: tokenizer/model were overwritten by FinBERT)
#-------------------------------------------
roberta_model_name = "cardiffnlp/twitter-roberta-base-sentiment-latest"

# This tokenizer/model belong ONLY to RoBERTa
roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_model_name)
roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_model_name).to("cuda")

# ------------------------------------------------------------
# 3) CLASSIFICATION FUNCTION (GPU)
# ------------------------------------------------------------
def classify_roberta(text):
    """
    IMPORTANT:
    This function must use the RoBERTa tokenizer and model.
    Before, FinBERT overwrote 'tokenizer' and 'model',
    causing both models to produce identical output.
    """
    inputs = roberta_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length"
    ).to("cuda")

    with torch.no_grad():
        logits = roberta_model(**inputs).logits

    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()
    labels = ["negative", "neutral", "positive"]
    return labels[probs.argmax()]

# 4) Convert Spark chunks â†’ Pandas
# FIXED: use a separate variable for RoBERTa
pdf_chunks_roberta = df_chunks.toPandas()

# 5) Run GPU sentiment for RoBERTa
roberta_sentiments = []

for text in pdf_chunks_roberta["chunk_text"]:
    roberta_sentiments.append(classify_roberta(text))

# Store results
pdf_chunks_roberta["sentiment"] = roberta_sentiments

# SAVE A COPY OF ROBERTA RESULTS
pdf_roberta = pdf_chunks_roberta.copy()

# ===============================================================
#  9 Computing Final Sentiment(aggregating chunks inference to one inference)
# ===============================================================
# Convert back to Spark
df_scored_roberta = spark.createDataFrame(pdf_chunks_roberta)

# Aggregate RoBERTa results
df_counts_roberta = (
    df_scored_roberta.groupBy("post_id")
    .pivot("sentiment", ["negative", "neutral", "positive"])
    .count()
    .na.fill(0)
)

df_final_roberta = df_counts_roberta.withColumn(
    "final_sentiment",
    F.when((F.col("negative") > F.col("neutral")) & (F.col("negative") > F.col("positive")), "negative")
     .when((F.col("positive") > F.col("neutral")) & (F.col("positive") > F.col("negative")), "positive")
     .otherwise("neutral")
)

df_final_roberta.show(5, truncate=False)

#------------------------------------------INSPECT ------------------------------

df_final_roberta.groupBy("final_sentiment").count().show()


#=============================================================
# 10: Dashboard Dataset - Time Adjustment 
#=======================================================


df_export = (
    df_final_roberta.join(
        df_ready.select("post_id", "subreddit", "created_utc"),
        on="post_id",
        how="left"
    )
)

# ------------Final Dataset: df_export time adjusted 
from pyspark.sql import functions as F

df_export = df_export.withColumn(
    "created_date",
    F.from_unixtime(F.col("created_utc")).cast("date")
)

df_export.show(20, truncate=False)

#=============================================================
# 11: Dashboard Dataset - Export as CSV 
#=============================================================

# using panda libary to export dataset to csv
import pandas as pd
# From Spark to pandas dataframe 
pdf_export = df_export.toPandas()

output_path = r"C:/Users/Rayane/OneDrive/Desktop/semster1/7012_pro/adjusted_time_sentiment_analysis.csv"
# File Exportation 
pdf_export.to_csv(output_path, index=False, encoding="utf-8")

#------------------------------------------- New MODEL ---------------------------------------------------------
# =================================================================
# 13:  New Load : Load FINBERT (GPU)
# =================================================================


from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch
import numpy as np

# FIXED: Use model-specific variable name
finbert_model_name = "yiyanghkust/finbert-tone"

# These belong ONLY to FinBERT
finbert_tokenizer = AutoTokenizer.from_pretrained(finbert_model_name)
finbert_model = AutoModelForSequenceClassification.from_pretrained(finbert_model_name).to("cuda")

# ------------------------------------------------------------
# Classification function for FinBERT
# ------------------------------------------------------------
def classify_finbert(text):
    """
    Uses the FinBERT tokenizer/model.
    Before: this function reused the same 'tokenizer' and 'model'
    used by RoBERTa, so results got mixed.
    """

    # Tokenize using *FinBERT tokenizer*
    inputs = finbert_tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length"
    ).to("cuda")

    # Run *FinBERT model*
    with torch.no_grad():
        logits = finbert_model(**inputs).logits

    probs = torch.softmax(logits, dim=1)[0].cpu().numpy()

    # FinBERT label order
    labels = ["positive", "neutral", "negative"]

    return labels[np.argmax(probs)]

# Convert Spark chunks â†’ Pandas
# FIXED: Use a separate dataframe name
pdf_chunks_finbert = df_chunks.toPandas()

# Run FinBERT sentiment classification
finbert_sentiments = []

for text in pdf_chunks_finbert["chunk_text"]:
    finbert_sentiments.append(classify_finbert(text))

pdf_chunks_finbert["sentiment"] = finbert_sentiments

# Save a copy BEFORE anything overwrites it
pdf_finbert = pdf_chunks_finbert.copy()





# ===============================================================
# 14 Computing Final Sentiment(aggregating chunks inference to one inference)
# ===============================================================
# Back to Spark
df_scored_finbert = spark.createDataFrame(pdf_chunks_finbert)

# Aggregate FinBERT results
df_counts_finbert = (
    df_scored_finbert.groupBy("post_id")
    .pivot("sentiment", ["negative", "neutral", "positive"])
    .count()
    .na.fill(0)
)

df_final_finbert = df_counts_finbert.withColumn(
    "final_sentiment",
    F.when((F.col("negative") > F.col("neutral")) & (F.col("negative") > F.col("positive")), "negative")
     .when((F.col("positive") > F.col("neutral")) & (F.col("positive") > F.col("negative")), "positive")
     .otherwise("neutral")
)

df_final_finbert.show(5, truncate=False)

df_final_finbert.groupBy("final_sentiment").count().show()
#===============================================
#  Date adjustment 
#================================================

#----------------------------------Final dataset with dates----------------------
# Join FINBERT results with original timestamps
df_export2 = (
    df_final_finbert
    .join(
        df_ready.select("post_id", "created_utc"),
        on="post_id",
        how="left"
    )
)

from pyspark.sql.functions import from_unixtime

df_export2 = df_export2.withColumn(
    "date",
    from_unixtime("created_utc").cast("timestamp")
)
#Convert timestamp to readable date
df_export2.show(20, truncate=False)




import pandas as pd
import matplotlib.pyplot as plt
from pyspark.sql import functions as F

# ===============================================================
# 1. Get sentiment counts from both FINAL Spark DataFrames
# ===============================================================

# FIXED: Use correct variables from your new pipeline
counts_roberta_spark = df_final_roberta.groupBy("final_sentiment").count()
counts_finbert_spark = df_final_finbert.groupBy("final_sentiment").count()

# ===============================================================
# 2. Convert Spark â†’ Pandas
# ===============================================================

counts_roberta = (
    counts_roberta_spark
    .toPandas()
    .set_index("final_sentiment")["count"]
)

counts_finbert = (
    counts_finbert_spark
    .toPandas()
    .set_index("final_sentiment")["count"]
)

# ===============================================================
# 3. Ensure consistent label ordering
# ===============================================================

sentiments = ["positive", "neutral", "negative"]

counts_roberta = counts_roberta.reindex(sentiments, fill_value=0)
counts_finbert = counts_finbert.reindex(sentiments, fill_value=0)

# ===============================================================
# 4. Combine into one DataFrame for plotting
# ===============================================================

df_compare = pd.DataFrame({
    "RoBERTa": counts_roberta,
    "FinBERT": counts_finbert
}, index=sentiments)

print(df_compare)

# ===============================================================
# 5. Plot grouped bar chart
# ===============================================================

ax = df_compare.plot(
    kind="bar",
    figsize=(10, 6),
    width=0.75,
    alpha=0.85,
    rot=0,
    color=["blue", "yellow"]
)

plt.title("Comparison of Sentiment Predictions (RoBERTa vs FinBERT)")
plt.ylabel("Number of Posts")
plt.xlabel("Sentiment Label")
plt.legend(title="Model")
plt.grid(axis="y", linestyle="--", alpha=0.5)

plt.show()

